{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d62db7ec-996c-4fe0-9f9a-bf873d0e3d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import gc\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from functools import lru_cache\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import timm\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import glob\n",
    "import datetime\n",
    "import argparse\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.distributed as dist\n",
    "from torch import nn, einsum\n",
    "from torch.nn import functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "from typing import Iterable, Optional\n",
    "from timm.models import create_model\n",
    "from timm.optim import create_optimizer\n",
    "from timm.scheduler import create_scheduler\n",
    "from timm.data import Mixup,create_transform\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.utils import accuracy, ModelEma,NativeScaler, get_state_dict, ModelEma\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets.folder import ImageFolder, default_loader\n",
    "\n",
    "from functools import partial\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648b5d94-7194-48f4-b063-f9186e49e604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2696c0c0-1907-4501-9676-3ca16a1abfd7",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87cd35fc-da9f-4b76-8f5f-05b59df5cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def concat_all_gather(tensor):\n",
    "    \"\"\"\n",
    "    Performs all_gather operation on the provided tensors.\n",
    "    *** Warning ***: torch.distributed.all_gather has no gradient.\n",
    "    \"\"\"\n",
    "    tensors_gather = [\n",
    "        torch.ones_like(tensor) for _ in range(torch.distributed.get_world_size())\n",
    "    ]\n",
    "    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\n",
    "\n",
    "    output = torch.cat(tensors_gather, dim=0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ff4a5-8fe9-4ae5-9bbc-91ee77295221",
   "metadata": {},
   "source": [
    "# build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff5caa6c-1e9a-4d8b-a69f-a35495eb89ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class E_MHSA(nn.Module):\n",
    "    \"\"\"\n",
    "    Efficient Multi-Head Self Attention\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, out_dim=None, head_dim=32, qkv_bias=True, qk_scale=None,\n",
    "                 attn_drop=0, proj_drop=0., sr_ratio=1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.out_dim = out_dim if out_dim is not None else dim\n",
    "        self.num_heads = self.dim // head_dim\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.q = nn.Linear(dim, self.dim, bias=qkv_bias)\n",
    "        self.k = nn.Linear(dim, self.dim, bias=qkv_bias)\n",
    "        self.v = nn.Linear(dim, self.dim, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(self.dim, self.out_dim)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.sr_ratio = sr_ratio\n",
    "        self.N_ratio = sr_ratio ** 2\n",
    "        if sr_ratio > 1:\n",
    "            self.sr = nn.AvgPool1d(kernel_size=self.N_ratio, stride=self.N_ratio)\n",
    "            self.norm = nn.BatchNorm1d(dim, eps=NORM_EPS)\n",
    "        self.is_bn_merged = False\n",
    "\n",
    "    def merge_bn(self, pre_bn):\n",
    "        merge_pre_bn(self.q, pre_bn)\n",
    "        if self.sr_ratio > 1:\n",
    "            merge_pre_bn(self.k, pre_bn, self.norm)\n",
    "            merge_pre_bn(self.v, pre_bn, self.norm)\n",
    "        else:\n",
    "            merge_pre_bn(self.k, pre_bn)\n",
    "            merge_pre_bn(self.v, pre_bn)\n",
    "        self.is_bn_merged = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        q = self.q(x)\n",
    "        q = q.reshape(B, N, self.num_heads, int(C // self.num_heads)).permute(0, 2, 1, 3)\n",
    "\n",
    "        if self.sr_ratio > 1:\n",
    "            x_ = x.transpose(1, 2)\n",
    "            x_ = self.sr(x_)\n",
    "            if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n",
    "                x_ = self.norm(x_)\n",
    "            x_ = x_.transpose(1, 2)\n",
    "            k = self.k(x_)\n",
    "            k = k.reshape(B, -1, self.num_heads, int(C // self.num_heads)).permute(0, 2, 3, 1)\n",
    "            v = self.v(x_)\n",
    "            v = v.reshape(B, -1, self.num_heads, int(C // self.num_heads)).permute(0, 2, 1, 3)\n",
    "        else:\n",
    "            k = self.k(x)\n",
    "            k = k.reshape(B, -1, self.num_heads, int(C // self.num_heads)).permute(0, 2, 3, 1)\n",
    "            v = self.v(x)\n",
    "            v = v.reshape(B, -1, self.num_heads, int(C // self.num_heads)).permute(0, 2, 1, 3)\n",
    "        attn = (q @ k) * self.scale\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13fe1dc9-cf39-4d58-8ae7-02b01fbeff5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NORM_EPS = 1e-5\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 stride=1,\n",
    "                 mode = \"V\"\n",
    "                ):\n",
    "        super(PatchEmbed, self).__init__()\n",
    "        norm_layer = partial(nn.BatchNorm2d, eps=NORM_EPS)\n",
    "        if stride == 4 and mode == \"V\":\n",
    "            self.avgpool = nn.AvgPool2d((4, 32), stride=4, ceil_mode=True, count_include_pad=False)\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "            self.norm = norm_layer(out_channels)\n",
    "        elif stride == 2 and mode == \"V\":\n",
    "            self.avgpool = nn.AvgPool2d((2, 16), stride=2, ceil_mode=True, count_include_pad=False)\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "            self.norm = norm_layer(out_channels)\n",
    "        elif stride == 2 and mode == \"H\":\n",
    "            self.avgpool = nn.AvgPool2d((64, 2), stride=2, ceil_mode=True, count_include_pad=False)\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "            self.norm = norm_layer(out_channels)\n",
    "        elif stride == 1 and mode == \"H\":\n",
    "            self.avgpool = nn.AvgPool2d((32, 1), stride=1, ceil_mode=True, count_include_pad=False)\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "            self.norm = norm_layer(out_channels)\n",
    "        elif in_channels != out_channels:\n",
    "            self.avgpool = nn.Identity()\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "            self.norm = norm_layer(out_channels)\n",
    "        else:\n",
    "            self.avgpool = nn.Identity()\n",
    "            self.conv = nn.Identity()\n",
    "            self.norm = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.norm(self.conv(self.avgpool(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "58389847-ba20-4e45-a3ee-0b721373b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class E_PQKV(nn.Module):\n",
    "    \"\"\"\n",
    "    product QKV\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, head_dim=32, out_dim=None, qkv_bias=True, qk_scale=None,\n",
    "                 attn_drop=0, proj_drop=0., sr_ratio=1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.out_dim = out_dim if out_dim is not None else dim\n",
    "        self.num_heads = self.dim // head_dim\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.q = nn.Linear(dim, self.dim, bias=qkv_bias)\n",
    "        self.k = nn.Linear(dim, self.dim, bias=qkv_bias)\n",
    "        self.v = nn.Linear(dim, self.dim, bias=qkv_bias)\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        q = self.q(x)\n",
    "        q = q.reshape(B, N, self.num_heads, int(C // self.num_heads)).permute(0, 2, 1, 3)\n",
    "        k = self.k(x)\n",
    "        k = k.reshape(B, -1, self.num_heads, int(C // self.num_heads)).permute(0, 2, 3, 1)\n",
    "        v = self.v(x)\n",
    "        v = v.reshape(B, -1, self.num_heads, int(C // self.num_heads)).permute(0, 2, 1, 3)\n",
    "        return q,k,v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "35e1a93a-e150-4a57-98f0-9e1f24a2391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class E_CA(nn.Module):\n",
    "    \"\"\"\n",
    "    product cross attention\n",
    "    \"\"\"\n",
    "    def __init__(self, dim,out_dim,qk_scale=None, attn_drop=0, proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.scale = qk_scale \n",
    "        self.proj = nn.Linear(dim,out_dim)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "    def forward(self, q,k,v,b,n,c):\n",
    "        attn = (q @ k) * self.scale\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        attn = (attn @ v).transpose(1, 2).reshape(b,n,c)\n",
    "        attn = self.proj(attn)\n",
    "        attn = self.proj_drop(attn)\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "7f8e4905-b5f0-436a-b0d5-fc9956f1ca3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class E_MTDA(nn.Module):\n",
    "    \"\"\"\n",
    "    Efficient Multi-Head Three-Dimensional Attention\n",
    "    x_mh,x_ah,x_mv,x_av\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, head_dim=32, out_dim=None, qkv_bias=True, qk_scale=None,\n",
    "                 attn_drop=0, proj_drop=0., sr_ratio=1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.out_dim = out_dim if out_dim is not None else dim\n",
    "        self.num_heads = self.dim // head_dim\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = E_PQKV( dim, head_dim, out_dim, qkv_bias, qk_scale,attn_drop, proj_drop, sr_ratio)\n",
    "        self.ca = E_CA(self.dim,self.out_dim,self.scale,attn_drop,proj_drop)\n",
    "        self.proj = nn.Linear(self.dim, self.out_dim)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.sr_ratio = sr_ratio\n",
    "        self.N_ratio = sr_ratio ** 2\n",
    "        if sr_ratio > 1:\n",
    "            self.sr = nn.AvgPool1d(kernel_size=self.N_ratio, stride=self.N_ratio)\n",
    "            self.norm = nn.BatchNorm1d(dim, eps=NORM_EPS)\n",
    "        self.is_bn_merged = False\n",
    "\n",
    "    def merge_bn(self, pre_bn):\n",
    "        merge_pre_bn(self.q, pre_bn)\n",
    "        if self.sr_ratio > 1:\n",
    "            merge_pre_bn(self.k, pre_bn, self.norm)\n",
    "            merge_pre_bn(self.v, pre_bn, self.norm)\n",
    "        else:\n",
    "            merge_pre_bn(self.k, pre_bn)\n",
    "            merge_pre_bn(self.v, pre_bn)\n",
    "        self.is_bn_merged = True\n",
    "    # def pro_pkv(x_mh)\n",
    "\n",
    "    def forward(self, x_mh,x_ah,x_mv,x_av):\n",
    "        B, N, C = x_mh.shape\n",
    "        q_mh,k_mh,v_mh = self.qkv(x_mh)\n",
    "        q_mv,k_mv,v_mv = self.qkv(x_mv)\n",
    "        q_ah,k_ah,v_ah = self.qkv(x_ah)\n",
    "        q_av,k_av,v_av = self.qkv(x_av)\n",
    "        attn_mhav=self.ca(q_mh,k_av,v_mh, B, N, C)\n",
    "        attn_ahmv=self.ca(q_ah,k_mv,v_ah, B, N, C)\n",
    "        attn_mvah=self.ca(q_mv,k_ah,v_mv, B, N, C)\n",
    "        attn_avmh=self.ca(q_av,k_mh,v_av, B, N, C)\n",
    "\n",
    "        attn_mhav+=x_mh\n",
    "        attn_ahmv+=x_ah\n",
    "        attn_mvah+=x_mv\n",
    "        attn_avmh+=x_av\n",
    "        \n",
    "        attn_mhav = attn_mhav.mean(1)\n",
    "        attn_ahmv = attn_ahmv.mean(1)\n",
    "        attn_mvah = attn_mvah.mean(1)\n",
    "        attn_avmh = attn_avmh.mean(1)\n",
    "        x = torch.concat([attn_mhav,attn_ahmv,attn_mvah,attn_avmh],dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "a9f8f8f3-57a8-4475-9dc4-f740cc3af196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShardExamine(nn.Module):\n",
    "    \n",
    "    def __init__(self,dim,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 num_head=64, \n",
    "                 qkv_bias=False, \n",
    "                 attn_drop=0.1, \n",
    "                 proj_drop=0.1,\n",
    "                 stride=1\n",
    "                ):\n",
    "        super().__init__()\n",
    "        # super(LocalCoccurrence, self).__init__()\n",
    "        self.PEV = PatchEmbed(in_channels,out_channels,stride*2,mode = \"V\")\n",
    "        self.PEH = PatchEmbed(in_channels,out_channels,stride,mode = \"H\")\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn  = E_MTDA(dim,num_head)\n",
    "\n",
    "    def forward(self, u_m, u_a):\n",
    "        u_mv = self.PEV(u_m)\n",
    "        u_av = self.PEV(u_a)\n",
    "        u_m = self.PEH(u_m)\n",
    "        u_a = self.PEH(u_a)\n",
    "        B,C,H,W = u_mv.shape\n",
    "        L = H*W\n",
    "        dim = C\n",
    "\n",
    "        u_mv = u_mv.reshape(B,dim,L).permute(0,2,1)\n",
    "        u_av = u_av.reshape(B,dim,L).permute(0,2,1)\n",
    "        u_m  = u_m.reshape(B,dim,L).permute(0,2,1)\n",
    "        u_a  = u_a.reshape(B,dim,L).permute(0,2,1)\n",
    "\n",
    "        u_mv = self.norm1(u_mv)\n",
    "        u_av = self.norm1(u_av)\n",
    "        u_m  = self.norm1(u_m)\n",
    "        u_a  = self.norm1(u_a)\n",
    "        x = self.attn(u_m, u_a,u_mv, u_av)# x_mh,x_ah,x_mv,x_av\n",
    "        # x_m, x_a = self.attn(u_m, u_a)\n",
    "        # x_mv, x_av = self.attn(u_mv, u_a)\n",
    "        # gap_m = x_m.mean(1)\n",
    "        # gap_a = x_a.mean(1)\n",
    "        \n",
    "        return  x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "168cd361-8b7c-442a-83b2-c824b8768d64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class basNet(nn.Module):\n",
    "    \"\"\"\n",
    "    only two viwe\n",
    "    \"\"\"\n",
    "    def load_pretrain(self, ):\n",
    "        return\n",
    "\n",
    "    def __init__(self,K=8192, m=0.999, T=0.07,dim=128,nc=512):\n",
    "        super(basNet, self).__init__()\n",
    "        # self.output_type = ['inference', 'loss']\n",
    "\n",
    "        self.K = K\n",
    "        self.m = m\n",
    "        self.T = T\n",
    "        self.encoder_q = timm.create_model ('efficientnetv2_m',\n",
    "                                          pretrained=False, \n",
    "                                          drop_rate = 0.2, \n",
    "                                          drop_path_rate = 0.1,\n",
    "                                          num_classes=nc\n",
    "                                         )\n",
    "        self.encoder_k = timm.create_model ('efficientnetv2_m',\n",
    "                                          pretrained=False, \n",
    "                                          drop_rate = 0.2, \n",
    "                                          drop_path_rate = 0.1,\n",
    "                                          num_classes=nc\n",
    "                                         )\n",
    "        \n",
    "        \n",
    "        self.att = ShardExamine(64,304,64)\n",
    "        # dim_mlp = self.encoder_q.fc.weight.shape[1]\n",
    "        self.encoder_q.fc = nn.Sequential(\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 128),\n",
    "        )\n",
    "        self.encoder_k.fc =nn.Sequential(\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 128),\n",
    "        )\n",
    "        for param_q, param_k in zip(\n",
    "            self.encoder_q.parameters(), self.encoder_k.parameters()\n",
    "        ):\n",
    "            param_k.data.copy_(param_q.data)  # initialize\n",
    "            param_k.requires_grad = False  # not update by gradient\n",
    "\n",
    "        # create the queue\n",
    "        self.register_buffer(\"queue\", torch.randn(dim, K))\n",
    "        self.queue = nn.functional.normalize(self.queue, dim=0)\n",
    "\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "        # dim = 1280\n",
    "\n",
    "        # self.lc  = LocalCoccurrence(dim)\n",
    "        # self.gl  = GlobalConsistency(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 128),\n",
    "        )#<todo> mlp needs to be deep if backbone is strong?\n",
    "        self.cancer = nn.Linear(128,1)\n",
    "        # self.labels = torch.zeros(logits.shape[0], dtype=torch.long).cuda()\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_key_encoder(self):\n",
    "        \"\"\"\n",
    "        Momentum update of the key encoder\n",
    "        \"\"\"\n",
    "        for param_q, param_k in zip(\n",
    "            self.encoder_q.parameters(), self.encoder_k.parameters()\n",
    "        ):\n",
    "            param_k.data = param_k.data * self.m + param_q.data * (1.0 - self.m)\n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, keys):\n",
    "        # gather keys before updating queue\n",
    "        # keys = concat_all_gather(keys)\n",
    "\n",
    "        batch_size = keys.shape[0]\n",
    "\n",
    "        ptr = int(self.queue_ptr)\n",
    "        assert self.K % batch_size == 0  # for simplicity\n",
    "\n",
    "        # replace the keys at ptr (dequeue and enqueue)\n",
    "        self.queue[:, ptr : ptr + batch_size] = keys.T\n",
    "        ptr = (ptr + batch_size) % self.K  # move pointer\n",
    "\n",
    "        self.queue_ptr[0] = ptr\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def _batch_shuffle_ddp(self, x):\n",
    "        \"\"\"\n",
    "        Batch shuffle, for making use of BatchNorm.\n",
    "        *** Only support DistributedDataParallel (DDP) model. ***\n",
    "        \"\"\"\n",
    "        # gather from all gpus\n",
    "        batch_size_this = x.shape[0]\n",
    "        x_gather = concat_all_gather(x)\n",
    "        batch_size_all = x_gather.shape[0]\n",
    "\n",
    "        num_gpus = batch_size_all // batch_size_this\n",
    "\n",
    "        # random shuffle index\n",
    "        idx_shuffle = torch.randperm(batch_size_all).cuda()\n",
    "\n",
    "        # broadcast to all gpus\n",
    "        torch.distributed.broadcast(idx_shuffle, src=0)\n",
    "\n",
    "        # index for restoring\n",
    "        idx_unshuffle = torch.argsort(idx_shuffle)\n",
    "\n",
    "        # shuffled index for this gpu\n",
    "        gpu_idx = torch.distributed.get_rank()\n",
    "        idx_this = idx_shuffle.view(num_gpus, -1)[gpu_idx]\n",
    "\n",
    "        return x_gather[idx_this], idx_unshuffle\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _batch_unshuffle_ddp(self, x, idx_unshuffle):\n",
    "        \"\"\"\n",
    "        Undo batch shuffle.\n",
    "        *** Only support DistributedDataParallel (DDP) model. ***\n",
    "        \"\"\"\n",
    "        # gather from all gpus\n",
    "        batch_size_this = x.shape[0]\n",
    "        x_gather = concat_all_gather(x)\n",
    "        batch_size_all = x_gather.shape[0]\n",
    "\n",
    "        num_gpus = batch_size_all // batch_size_this\n",
    "\n",
    "        # restored index for this gpu\n",
    "        gpu_idx = torch.distributed.get_rank()\n",
    "        idx_this = idx_unshuffle.view(num_gpus, -1)[gpu_idx]\n",
    "\n",
    "        return x_gather[idx_this]\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            im_q: a batch of query images\n",
    "            im_k: a batch of key images\n",
    "        Output:\n",
    "            logits, targets\n",
    "        \"\"\"\n",
    "        features=[]\n",
    "        def hook(module, input, output):\n",
    "            features.append(input)\n",
    "            return None\n",
    "        self.encoder_q.blocks[4].register_forward_hook(hook)\n",
    "        self.encoder_q.blocks[5].register_forward_hook(hook)\n",
    "        self.encoder_q.blocks[6].register_forward_hook(hook)\n",
    "        batch_size,C,H,W = x.shape\n",
    "        x = x.reshape(-1, C, H, W)\n",
    "        x_m =torch.tensor( np.array_split(batch,2,axis=3)[0])\n",
    "\n",
    "        # print(x_m.shape)\n",
    "        x_a = torch.tensor( np.array_split(batch,2,axis=3)[1])\n",
    "        \n",
    "        \n",
    "        # compute query features\n",
    "        q_m = self.encoder_q(x_m)  # queries: NxC\n",
    "        q_a = self.encoder_q(x_a)  # queries: NxC\n",
    "        q_m = nn.functional.normalize(q_m, dim=1)\n",
    "        q_a = nn.functional.normalize(q_a, dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        last = torch.cat([q_a, q_m ],-1)\n",
    "        last = self.mlp(last)\n",
    "        \n",
    "        cancer = self.cancer(last).reshape(-1)\n",
    "        cancer = torch.sigmoid(cancer)\n",
    "        q_m = self.encoder_q.fc(q_m)\n",
    "        q_a = self.encoder_q.fc(q_a)\n",
    "        # compute key features\n",
    "        with torch.no_grad():  # no gradient to keys\n",
    "            self._momentum_update_key_encoder()  # update the key encoder\n",
    "            k_m = self.encoder_k(x_m)  # keys: NxC\n",
    "            k_m = nn.functional.normalize(k_m, dim=1)\n",
    "            k_m = self.encoder_k.fc(k_m) \n",
    "            k_a = self.encoder_k(x_a)  # keys: NxC\n",
    "            k_a = nn.functional.normalize(k_a, dim=1)\n",
    "            k_a = self.encoder_k.fc(k_a)\n",
    "\n",
    "        # compute logits\n",
    "        # Einstein sum is more intuitive\n",
    "        # positive logits: Nx1\n",
    "        l_pos1 = torch.einsum(\"nc,nc->n\", [q_m, k_m]).unsqueeze(-1)\n",
    "        l_pos2 = torch.einsum(\"nc,nc->n\", [q_m, k_a]).unsqueeze(-1)\n",
    "        l_pos3 = torch.einsum(\"nc,nc->n\", [q_a, k_m]).unsqueeze(-1)\n",
    "        l_pos4 = torch.einsum(\"nc,nc->n\", [q_a, k_a]).unsqueeze(-1)\n",
    "        # negative logits: NxK\n",
    "        l_neg_m = torch.einsum(\"nc,ck->nk\", [q_m, self.queue.clone().detach()])\n",
    "        l_neg_a = torch.einsum(\"nc,ck->nk\", [q_a, self.queue.clone().detach()])\n",
    "        # logits: Nx(1+K)\n",
    "        logits = torch.cat([l_pos1,l_pos2,l_pos3,l_pos4, l_neg_m,l_neg_a], dim=1)\n",
    "\n",
    "        # apply temperature\n",
    "        logits /= self.T\n",
    "\n",
    "        # labels: positive key indicators\n",
    "        labels = torch.zeros(logits.shape[1], dtype=torch.long).cuda()\n",
    "        labels[1],labels[2],labels[3]=1,2,3\n",
    "        \n",
    "        \n",
    "        # dequeue and enqueue\n",
    "        self._dequeue_and_enqueue(k_m)\n",
    "        self._dequeue_and_enqueue(k_a)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return cancer,logits, labels,features\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0c3e1-b63e-4f7a-81c7-5500427a442b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### torch.Size([1, 160, 64, 32])\n",
    "### torch.Size([1, 176, 64, 32])\n",
    "### torch.Size([1, 304, 32, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "cca0d2bd-f350-45d0-a28f-3207915ce007",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ShardExamine(64,304,64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "6c719fee-ec14-41e9-8415-e0595e101a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 304, 32, 16])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_m = np.random.random(size=(2, 304, 32, 16))\n",
    "i_a = np.random.random(size=(2, 304, 32, 16))\n",
    "i_m = torch.tensor(i_m).to(device).to(torch.float32)\n",
    "i_a = torch.tensor(i_a).to(device).to(torch.float32)\n",
    "i_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f0751605-eb5e-4013-9295-f876623e3872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "e= net(i_m,i_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "973e4c60-89ac-4019-9ba2-4ea4a6727e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "df3b0716-b80b-4895-8eed-27c5099740ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 64])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c2958993-8548-4e77-b4e0-ff26fdc429cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d692a080-8638-4a35-9e2b-bc37a475760c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 64])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "818db654-0385-47e1-b605-7a92735c6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PatchEmbed(160,32,1,\"H\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3795db21-a309-412e-887c-4f19529f1a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 1, 16])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = pe(i_a).to(device)\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "48dcc562-019c-46cd-b64a-6e34643de6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "B,C,H,W = xx.shape\n",
    "L = H*W\n",
    "dim = C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b700c6bc-62b8-4f2b-bb67-f9a97049d77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 32])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = xx.reshape(B,dim,L).permute(0,2,1)\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0820355-9feb-434c-aff6-c2707e37bd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.LayerNorm(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab8c19-5a9b-476f-adc8-edcae4e86014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
